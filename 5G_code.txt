import numpy as np
import pandas as pd
import keras
from keras import layers
from matplotlib import pyplot as plt

# Define the number of time steps
TIME_STEPS = 12  # Example value, adjust as needed

file_train_root = "E:/Users/Δημήτρης Τσάμπρας/Desktop/5G/DATA/train_formated.csv"
file_test_root = "E:/Users/Δημήτρης Τσάμπρας/Desktop/5G/DATA/test_formated.csv"
#
# fields = ['var1', 'var2', 'var3', 'var4', 'var5', 'var6', 'var7',
#         'var8', 'var9', 'var10', 'var11', 'var12', 'var13', 'var14',
#         'var15', 'var16', 'var17', 'var18', 'var19', 'var20',
#          'var21', 'var22', 'var23', 'var24', 'var25', 'var26',
#          'var27', 'var28', 'var29', 'var30', 'var31', 'var32',
#          'var33', 'var34', 'var35', 'var36', 'var37', 'var38']

# read only the specified column
# read and display var1

df_0 = pd.read_csv(file_train_root, usecols=[0])
print(df_0)

# read and display var 2
df_1 = pd.read_csv(file_train_root, usecols=[1])
print(df_1)

df_testset0 = pd.read_csv(file_test_root, usecols=[0])
print(df_testset0)

# df_testset1 = pd.read_csv(file_test_root, usecols=[1])
# print(df_testset1)

print(df_testset0.dtypes)
df_testset0['var1'] = pd.to_numeric(df_testset0['var1'], errors='coerce')

# plot var1
fig, ax = plt.subplots()
df_0.plot(legend=False, ax=ax)
plt.show()

# plot var2
fig, ax = plt.subplots()
df_1.plot(legend=False, ax=ax)
plt.show()

# plot var1 test
fig, ax = plt.subplots()
df_testset0.plot(legend=False, ax=ax)
plt.show()

# plot var2 test
# fig, ax = plt.subplots()
# df_testset1.plot(legend=False, ax=ax)
# plt.show()

# var 1
training_mean = df_0.mean()
training_std = df_0.std()
df_training_value = (df_0 - training_mean) / training_std
print("Number of training samples:", len(df_training_value))


# Generated training sequences for use in the model.
def create_sequences(values, time_steps=TIME_STEPS):
    output = []
    for i in range(len(values) - time_steps + 1):
        output.append(values[i: (i + time_steps)])
    return np.stack(output)


x_train = create_sequences(df_training_value.values)
print("Training input shape: ", x_train.shape)

model = keras.Sequential(
    [
        layers.Input(shape=(x_train.shape[1], x_train.shape[2])),
        layers.Conv1D(
            filters=32,
            kernel_size=7,
            padding="same",
            strides=2,
            activation="relu",
        ),
        layers.Dropout(rate=0.2),
        layers.Conv1D(
            filters=16,
            kernel_size=7,
            padding="same",
            strides=2,
            activation="relu",
        ),
        layers.Conv1DTranspose(
            filters=16,
            kernel_size=7,
            padding="same",
            strides=2,
            activation="relu",
        ),
        layers.Dropout(rate=0.2),
        layers.Conv1DTranspose(
            filters=32,
            kernel_size=7,
            padding="same",
            strides=2,
            activation="relu",
        ),
        layers.Conv1DTranspose(filters=1, kernel_size=7, padding="same"),
    ]
)
model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss="mse")
model.summary()

history = model.fit(
    x_train,
    x_train,
    epochs=50,
    batch_size=128,
    validation_split=0.1,
    callbacks=[
        keras.callbacks.EarlyStopping(monitor="val_loss", patience=5, mode="min")
    ],
)

plt.plot(history.history["loss"], label="Training Loss")
plt.plot(history.history["val_loss"], label="Validation Loss")
plt.legend()
plt.show()

# Get train MAE loss.
x_train_pred = model.predict(x_train)
train_mae_loss = np.mean(np.abs(x_train_pred - x_train), axis=1)

plt.hist(train_mae_loss, bins=50)
plt.xlabel("Train MAE loss")
plt.ylabel("No of samples")
plt.show()

# Get reconstruction loss threshold.
threshold = np.max(train_mae_loss)
print("Reconstruction error threshold: ", threshold)

# Checking how the first sequence is learnt
plt.plot(x_train[0])
plt.plot(x_train_pred[0])
plt.show()

df_test_value = (df_testset0 - training_mean) / training_std
fig, ax = plt.subplots()
df_test_value.plot(legend=False, ax=ax)
plt.show()

# Create sequences from test values.
x_test = create_sequences(df_test_value.values)
print("Test input shape: ", x_test.shape)

# Get test MAE loss.
x_test_pred = model.predict(x_test)
test_mae_loss = np.mean(np.abs(x_test_pred - x_test), axis=1)
test_mae_loss = test_mae_loss.reshape((-1))

plt.hist(test_mae_loss, bins=50)
plt.xlabel("test MAE loss")
plt.ylabel("No of samples")
plt.show()

# Detect all the samples which are anomalies.
anomalies = test_mae_loss > threshold
print("Number of anomaly samples: ", np.sum(anomalies))
print("Indices of anomaly samples: ", np.where(anomalies))

# data i is an anomaly if samples [(i - timesteps + 1) to (i)] are anomalies
anomalous_data_indices = []
for data_idx in range(TIME_STEPS - 1, len(df_test_value) - TIME_STEPS + 1):
    if np.all(anomalies[data_idx - TIME_STEPS + 1: data_idx]):
        anomalous_data_indices.append(data_idx)

df_subset = df_testset0.iloc[anomalous_data_indices]
fig, ax = plt.subplots()
df_testset0.plot(legend=False, ax=ax)
df_subset.plot(legend=False, ax=ax, color="r")
plt.show()
